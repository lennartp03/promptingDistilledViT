{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Collab specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was executed in a Google Collab environment, and it is recommended to run it in the same environment. To do so, the following cell contents need to be uncommented and executed.\n",
    "\n",
    "In case you want to run it locally, you will need to create a virtual environment and install the required packages. The `requirements.txt` file is provided in the repository. Some file and module imports may need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch torchvision avalanche-lib tqdm timm tensorflow_addons --quiet\n",
    "#! pip install tfds-nightly==4.4.0.dev202201080107 --quiet\n",
    "\n",
    "#! git clone https://github.com/lennartp03/promptingDistilledViT.git\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from avalanche.evaluation.metrics.accuracy import Accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "from timm.models.layers import DropPath\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler\n",
    "\n",
    "from src.utils.helpers import set_seed\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ./models/\n",
    "%mkdir ./models/convpass/\n",
    "%mkdir ./models/head/\n",
    "%mkdir ./models/full/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(method, dataset, modelname, model, acc, ep):\n",
    "    model.cpu()\n",
    "    save_path = f'./models/{method}/{modelname}_{dataset}.pt'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    log_path = f'./models/{method}/{modelname}_{dataset}.log'\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.write(f'{ep} {acc}\\n')\n",
    "\n",
    "def load(method, dataset, modelname, model):\n",
    "    load_path = f'./models/{method}/{modelname}_{dataset}.pt'\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model.cpu()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training details\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "epoch = 100\n",
    "\n",
    "def train(model, dl,\n",
    "          val_dl, dataset, modelname, method,\n",
    "          opt, scheduler, epoch = 100):\n",
    "    model.train()\n",
    "    model = model.cuda()\n",
    "    best_acc = 0\n",
    "    for ep in tqdm(range(epoch)):\n",
    "        model.train()\n",
    "        model = model.cuda()\n",
    "        for i, batch in enumerate(dl):\n",
    "            x, y = batch[0].cuda(), batch[1].cuda()\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(ep)\n",
    "        if ep % 10 == 9:\n",
    "            acc, _, _ = test(model, val_dl)\n",
    "            print('Best Acc: ', best_acc, ' Current Acc: ', acc)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                print('New Best Acc: ', best_acc)\n",
    "                save(method, dataset, modelname, model, acc, ep)\n",
    "    model = model.cpu()\n",
    "    return model, best_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dl, topn=5):\n",
    "    model.eval()\n",
    "    acc = Accuracy()\n",
    "    total_time = 0\n",
    "    top5, total = 0, 0\n",
    "    model = model.cuda()\n",
    "    for batch in dl:  \n",
    "        x, y = batch[0].cuda(), batch[1].cuda()\n",
    "        start_time = time.time()\n",
    "        out = model(x).data\n",
    "        inference_time = time.time() - start_time\n",
    "        total_time += inference_time\n",
    "\n",
    "        _, pred = out.topk(topn, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(y.view(1, -1).expand_as(pred))\n",
    "        top5 += correct[:topn].reshape(-1).float().sum(0, keepdim=True)\n",
    "        total += y.size(0)\n",
    "\n",
    "        acc.update(out.argmax(dim=1).view(-1), y)\n",
    "\n",
    "    print(acc.result())\n",
    "    top5_acc = top5 / total\n",
    "    mean_inference_time = total_time / len(dl)\n",
    "\n",
    "    return acc.result(), mean_inference_time, top5_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIT_BASE = 'vit_base_patch16_224' #768\n",
    "VIT_SMALL = 'vit_small_patch16_224' #384\n",
    "VIT_TINY = 'vit_tiny_patch16_224' #192\n",
    "DEIT_SMALL = 'deit_small_distilled_patch16_224' #384\n",
    "DEIT_TINY = 'deit_tiny_distilled_patch16_224' #192\n",
    "\n",
    "MODEL = 'vit_tiny_patch16_224'\n",
    "TOP_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ./data/\n",
    "%mkdir ./data/stanford_cars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Collab Commands for adding Stanford Cars Dataset from Google Drive\n",
    "#!cp -r drive/MyDrive/Bachelor/stanfordcars/devkit/ data/stanford_cars/\n",
    "#!cp -r drive/MyDrive/Bachelor/stanfordcars/cars_test_annos_withlabels.mat data/stanford_cars\n",
    "\n",
    "#!unzip drive/MyDrive/Bachelor/stanfordcars/archive.zip -d data/stanford_cars/\n",
    "\n",
    "#!mv data/stanford_cars/cars_test/cars_test/* data/stanford_cars/cars_test/\n",
    "#!mv data/stanford_cars/cars_train/cars_train/* data/stanford_cars/cars_train/\n",
    "\n",
    "#!rmdir data/stanfordcars/cars_test/cars_test/\n",
    "#!rmdir data/stanfordcars/cars_train/cars_train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGVC - Convpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fgvc.blueprint import FGVCDataPytorch\n",
    "from src.convpass.utils import set_Convpass\n",
    "\n",
    "DATASET_LIST = {\n",
    "    'Aircraft': 100,\n",
    "    'Food101': 101,\n",
    "    'Flowers': 102,\n",
    "    'Pets': 37,\n",
    "    'StanfordCars': 196,\n",
    "}\n",
    "\n",
    "SCALING_PARAM = 1\n",
    "SAMPLE_LIST = [2,4,8,16]\n",
    "\n",
    "for DATA in DATASET_LIST.keys():\n",
    "  print(DATA, DATASET_LIST[DATA])\n",
    "  for num in SAMPLE_LIST:\n",
    "\n",
    "    fgvc = FGVCDataPytorch(dataset=DATA, samples_per_class=num, pin_memory=False)\n",
    "    train_fgvc, val_fgvc, test_fgvc = fgvc.get_loaders()\n",
    "\n",
    "    train_loader = train_fgvc\n",
    "    val_loader = val_fgvc\n",
    "    test_loader = test_fgvc\n",
    "\n",
    "    model = create_model(MODEL, pretrained=True,\n",
    "                        drop_path_rate=0.1)\n",
    "    set_Convpass(model, distilled=True, adapt_dim=192, s=SCALING_PARAM, xavier_init=True)\n",
    "\n",
    "    trainable = []\n",
    "    model.reset_classifier(DATASET_LIST[DATA])\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'adapter' in n or 'head' in n:\n",
    "            trainable.append(p)\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "    scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                      warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "    model_trained, acc = train(model, train_fgvc, val_fgvc,\n",
    "                                DATA, MODEL, \"convpass\", opt, scheduler)\n",
    "\n",
    "    model_trained_best = load(\"convpass\", DATA, MODEL, model)\n",
    "    acc, inference_mean, top5_acc = test(model_trained_best, test_fgvc, TOP_N)\n",
    "    print('Num samples per class:', num)\n",
    "    print('Accuracy:', acc, '\\nInference:', inference_mean, '\\nTop5Acc:', top5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGVC - Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fgvc.blueprint import FGVCDataPytorch\n",
    "\n",
    "DATASET_LIST = {\n",
    "    'Aircraft': 100,\n",
    "    'Food101': 101,\n",
    "    'Flowers': 102,\n",
    "    'Pets': 37,\n",
    "    'StanfordCars': 196,\n",
    "}\n",
    "\n",
    "SCALING_PARAM = 1\n",
    "SAMPLE_LIST = [2,4,8,16]\n",
    "\n",
    "for DATA in DATASET_LIST.keys():\n",
    "  print(DATA, DATASET_LIST[DATA])\n",
    "  for num in SAMPLE_LIST:\n",
    "\n",
    "    fgvc = FGVCDataPytorch(dataset=DATA, samples_per_class=num, pin_memory=False)\n",
    "    train_fgvc, val_fgvc, test_fgvc = fgvc.get_loaders()\n",
    "\n",
    "    print(len(train_fgvc.dataset), len(val_fgvc.dataset), len(test_fgvc.dataset))\n",
    "\n",
    "    model_head = create_model(MODEL, pretrained=True,\n",
    "                        drop_path_rate=0.1)\n",
    "\n",
    "    trainable = []\n",
    "    model_head.reset_classifier(DATASET_LIST[DATA])\n",
    "\n",
    "    for n, p in model_head.named_parameters():\n",
    "      if 'head' in n:\n",
    "        trainable.append(p)\n",
    "      else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "    opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "    scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                      warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "    model_head_trained, acc = train(model_head, train_fgvc, val_fgvc,\n",
    "                                DATA, MODEL, \"head\", opt, scheduler)\n",
    "\n",
    "    model_head_trained_best = load(\"head\", DATA, MODEL, model_head)\n",
    "    acc, inference_mean, top5_acc = test(model_head_trained_best, test_fgvc, TOP_N)\n",
    "\n",
    "    print('Accuracy:', acc, '\\nInference:', inference_mean, '\\nTop5Acc:', top5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google collab command for manually transferring RESISC45 dataset from Google Drive\n",
    "# !mkdir -p /content/testing/src/data/datasets/downloads/manual/resisc45\n",
    "# !unrar x drive/MyDrive/Bachelor/resisc45/NWPU-RESISC45.rar /content/testing/src/data/datasets/downloads/manual/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader import construct_train_loader, construct_test_loader, construct_val_loader\n",
    "from src.convpass.utils import set_Convpass\n",
    "\n",
    "DATASETS = {\n",
    "    'cifar(num_classes=100)': {'num_classes': 100, 'scaling_param': 0.1, 'xavier_init': False},\n",
    "    'dtd': {'num_classes': 47, 'scaling_param': 0.01, 'xavier_init': True},\n",
    "    'oxford_flowers102': {'num_classes': 102, 'scaling_param': 0.1, 'xavier_init': True},\n",
    "    'patch_camelyon': {'num_classes': 2, 'scaling_param': 10, 'xavier_init': False},\n",
    "    'resisc45': {'num_classes': 45, 'scaling_param': 10, 'xavier_init': False},\n",
    "    'eurosat': {'num_classes': 10, 'scaling_param': 10, 'xavier_init': False},\n",
    "    'kitti(task=\"closest_vehicle_distance\")': {'num_classes': 4, 'scaling_param': 10, 'xavier_init': True},\n",
    "    'smallnorb(predicted_attribute=\"label_elevation\")': {'num_classes': 9, 'scaling_param': 1, 'xavier_init': True},\n",
    "    'clevr(task=\"count_all\")': {'num_classes': 8, 'scaling_param': 1, 'xavier_init': False},\n",
    "}\n",
    "\n",
    "DATASET_NAME = 'smallnorb(predicted_attribute=\"label_elevation\")'\n",
    "DATA_PATH = './src/data/datasets/'\n",
    "NUM_CLS = DATASETS[DATASET_NAME]['num_classes']\n",
    "SCALE_PARAM = DATASETS[DATASET_NAME]['scaling_param']\n",
    "XAVIER_INIT = DATASETS[DATASET_NAME]['xavier_init']\n",
    "TOP_N = 2\n",
    "\n",
    "print(DATASET_NAME, NUM_CLS, SCALE_PARAM)\n",
    "\n",
    "train_loader = construct_train_loader(f'vtab-{DATASET_NAME}', DATA_PATH, NUM_CLS, pin_memory=False)\n",
    "val_loader = construct_val_loader(f'vtab-{DATASET_NAME}', DATA_PATH, NUM_CLS, pin_memory=False)\n",
    "test_loader = construct_test_loader(f'vtab-{DATASET_NAME}', DATA_PATH, NUM_CLS, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIT_BASE = 'vit_base_patch16_224' #768\n",
    "VIT_SMALL = 'vit_small_patch16_224' #384\n",
    "VIT_TINY = 'vit_tiny_patch16_224' #192\n",
    "DEIT_SMALL = 'deit_small_distilled_patch16_224' #384\n",
    "DEIT_TINY = 'deit_tiny_distilled_patch16_224' #192\n",
    "\n",
    "MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTAB - Convpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: ', len(train_loader.dataset), 'Val: ', len(val_loader.dataset),\n",
    "      'Test: ', len(test_loader.dataset))\n",
    "\n",
    "model_convpass = create_model(MODEL, pretrained=True,\n",
    "                      drop_path_rate=0.1)\n",
    "set_Convpass(model_convpass, distilled=True, adapt_dim=192, s=SCALE_PARAM, xavier_init=XAVIER_INIT)\n",
    "\n",
    "trainable = []\n",
    "model_convpass.reset_classifier(NUM_CLS)\n",
    "\n",
    "for n, p in model_convpass.named_parameters():\n",
    "    if 'adapter' in n or 'head' in n:\n",
    "        trainable.append(p)\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                    warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model_convpass_trained, acc = train(model_convpass, train_loader, val_loader,\n",
    "                                    DATASET_NAME, MODEL, \"convpass\", opt, scheduler)\n",
    "\n",
    "model_convpass_trained_best = load(\"convpass\", DATASET_NAME, MODEL, model_convpass)\n",
    "acc, inference_mean, top5_acc = test(model_convpass_trained_best, test_loader, TOP_N)\n",
    "\n",
    "print('Accuracy:', acc, '\\nInference:', inference_mean, '\\nTop5Acc:', top5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTAB - Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head = create_model(MODEL, pretrained=True,\n",
    "                      drop_path_rate=0.1)\n",
    "model_head.reset_classifier(NUM_CLS)\n",
    "\n",
    "trainable = []\n",
    "\n",
    "for n, p in model_head.named_parameters():\n",
    "    if 'head' in n:\n",
    "        trainable.append(p)\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "opt = AdamW(trainable, lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                    warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model_head_trained, acc = train(model_head, train_loader, val_loader,\n",
    "                                DATASET_NAME, MODEL, \"head\", opt, scheduler)\n",
    "\n",
    "model_head_trained_best = load(\"head\", DATASET_NAME, MODEL, model_head)\n",
    "acc, inference_mean, top5_acc = test(model_head_trained_best, test_loader, TOP_N)\n",
    "\n",
    "print('Accuracy:', acc, '\\nInference:', inference_mean, '\\nTop5Acc:', top5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTAB - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = create_model(MODEL, pretrained=True,\n",
    "                     drop_path_rate=0.1)\n",
    "model_full.reset_classifier(NUM_CLS)\n",
    "\n",
    "for n, p in model_full.named_parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "opt = AdamW(model_full.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = CosineLRScheduler(opt, t_initial=100,\n",
    "                                  warmup_t=10, lr_min=1e-5, warmup_lr_init=1e-6)\n",
    "\n",
    "\n",
    "model_full_trained, acc = train(model_full, train_loader, val_loader,\n",
    "                                DATASET_NAME, MODEL, \"full\", opt, scheduler)\n",
    "\n",
    "model_full_trained_best = load(\"full\", DATASET_NAME, MODEL, model_head)\n",
    "acc, inference_mean, top5_acc = test(model_full_trained_best, test_loader, TOP_N)\n",
    "\n",
    "print('Accuracy:', acc, '\\nInference:', inference_mean, '\\nTop5Acc:', top5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helpers import count_total_params, count_finetuned_params\n",
    "\n",
    "MODEL = None\n",
    "\n",
    "total = count_total_params(MODEL)\n",
    "tuned = count_finetuned_params(MODEL)\n",
    "share = tuned/total\n",
    "\n",
    "print(f\"Number of parameters fine-tuned: {tuned}\")\n",
    "print(f\"Total number of parameters: {total}\")\n",
    "print(f\"Share: {share}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
